{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 퍼셉트론(Perceptron) 개념에 대해 설명하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 퍼셉트론이란 인간의 뉴런을 모방한 구조로 초기에는 간단한 or 이나 not 계산만 가능했습니다.  \n",
    "하지만, 여러 연구와 노력 끝에 여러개의 퍼셉트론과 층(Layer)을 사용하여 인간과 비슷한 사고 체계를 갖추는데 성공하였고 다양한 딥러닝 분야에서 강력한 성능을 보여주고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 퍼셉트론(Perceptron) 기본 동작 원리 및 수식을 도식화와 함께 작성해주세요.\n",
    "- 조건 : 피쳐 4개, 퍼셉트론 1개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 퍼셉트론은 각각의 피쳐와 피쳐가 가지고 있는 가중치를 곱한 것을 절편과 함께 모두 더한 뒤에 활성화 함수를 거쳐 출력으로 내보냅니다.  \n",
    "f : 피쳐, W : 가중치, b : 절편, act_func : 활성화 함수  \n",
    "act_func(f1W1 + f2W2 + f3W3 + f4W4 + b) ==> 출력  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 활성화함수(Activation Function)의 역할을 설명하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 활성화함수는 출력값의 범위를 일정 범위로 맞추거나 기울기 소실 문제를 해결하기 위해 사용합니다.  \n",
    "예를 들어 이진 분류에서 출력층의 활성화함수로 시그모이드(Sigmoid) 함수를 사용하는데 이 함수를 통해 출력값의 범위를 0~1로 맞추어 확률값으로 해석 가능하게 해줍니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 대표적인 활성화함수(Activation Function)에 대해 설명하세요.\n",
    "- 최소 4개 이상, 값의 범위"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 계단(Step) 함수 : 계단 함수는 딥러닝 초기에 주로 사용된 함수로 x가 0 미만일때는 0의 값을, 0 이상일때는 1의 값을 갖도록 하는 활성화 함수입니다.\n",
    "> 2. 시그모이드(Sigmoid) 함수 : 시그모이드 함수는 계단 함수를 발전시킨 함수로 좀 더 부드러운 곡선 형태를 가지며 이진 분류에서 주로 사용하며 출력값의 범위를 0~1 사이로 맞추어 확률값으로 해석 가능하게 해줍니다.\n",
    "> 3. 소프트맥스(Softmax) 함수 : 소프트맥스 함수는 다중분류에서 사용되며 시그모이드 함수를 통해 계산된 여러 개의 이진 분류 확률값의 합들을 1로 맞추기 위해 사용하며 범위는 0~1 사이를 갖습니다.\n",
    "> 3. 렐루(ReLU) 함수 : 렐루 함수는 입력층과 은닉층에서 주로 사용하며 역전파 과정에서 미분 시에 발생하는 기울기 소실 문제를 해결하기 위해 고안된 함수이며 x가 0 미만일때는 0의 값을, 0 이상일때는 x값을 갖습니다.\n",
    "> 4. Leaky 렐루(Leaky ReLU) 함수 : Leaky 렐루 함수는 렐루 함수에서 완전히 해결되지 못한 기울기 소실 문제를 조금 더 해결하기 위해 고안된 함수로 x가 0 미만일때 0의 값이 아닌 약간의 경사를 주어 아주 작은 음수 값을 갖도록 만든 함수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 경사 하강법의 개념 및 대표적인 경사하강법 알고리즘에 대해 간략히 설명하세요.\n",
    "- 최소 3개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 경사 하강법은 손실 함수의 미분을 통해 기울기를 계산하고 다음 이동할 위치를 계속해서 찾아나가 손실 함수가 최소값을 갖는 지점을 찾는 방법으로 종류로는 SGD, RMSprop, Adagrad, Adam 등이 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 회귀 모델 구현을 간략하게 코드 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_in, output_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(input_in, 30)\n",
    "        self.hidden_layer = nn.Linear(30, 10)\n",
    "        self.output_layer = nn.Linear(10, output_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력층\n",
    "        x = self.input_layer(x)\n",
    "        x = F.relu(x)\n",
    "        # 은닉층\n",
    "        x = self.hidden_layer(x)\n",
    "        x = F.relu(x)\n",
    "        # 출력층\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionModel(\n",
       "  (input_layer): Linear(in_features=3, out_features=30, bias=True)\n",
       "  (hidden_layer): Linear(in_features=30, out_features=10, bias=True)\n",
       "  (output_layer): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = RegressionModel(input_in=3, output_out=1)\n",
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 이진분류 모델 구현을 간략하게 코드 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryModel(nn.Module):\n",
    "    def __init__(self, input_in, output_out, hidden_list):\n",
    "        super().__init__()\n",
    "        # 입력층\n",
    "        self.input_layer = nn.Linear(input_in, hidden_list[0])\n",
    "        # 은닉층\n",
    "        self.hidden_layer_list = nn.ModuleList()\n",
    "        for i in range(len(hidden_list)-1):\n",
    "            self.hidden_layer_list.append(nn.Linear(hidden_list[i], hidden_list[i+1]))\n",
    "        # 출력층\n",
    "        self.output_layer = nn.Linear(hidden_list[-1], output_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력층\n",
    "        x = self.input_layer(x)\n",
    "        x = F.relu(x)\n",
    "        # 은닉층\n",
    "        for i in self.hidden_layer_list:\n",
    "            x = i(x)\n",
    "            x = F.relu(x)\n",
    "        # 출력층\n",
    "        x = self.output_layer(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryModel(\n",
       "  (input_layer): Linear(in_features=5, out_features=100, bias=True)\n",
       "  (hidden_layer_list): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=80, bias=True)\n",
       "    (1): Linear(in_features=80, out_features=60, bias=True)\n",
       "    (2): Linear(in_features=60, out_features=40, bias=True)\n",
       "    (3): Linear(in_features=40, out_features=20, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = BinaryModel(input_in=5, output_out=1, hidden_list=[100,80,60,40,20])\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 다중분류 모델 구현을 간략하게 코드 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassModel(nn.Module):\n",
    "    def __init__(self, input_in, output_out, hidden_list):\n",
    "        super().__init__()\n",
    "        # 입력층\n",
    "        self.input_layer = nn.Linear(input_in, hidden_list[0])\n",
    "        # 은닉층\n",
    "        self.hidden_layer_list = nn.ModuleList()\n",
    "        for i in range(len(hidden_list)-1):\n",
    "            self.hidden_layer_list.append(nn.Linear(hidden_list[i], hidden_list[i+1]))\n",
    "        # 출력층\n",
    "        self.output_layer = nn.Linear(hidden_list[-1], output_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력층\n",
    "        x = self.input_layer(x)\n",
    "        x = F.relu(x)\n",
    "        # 은닉층\n",
    "        for i in self.hidden_layer_list:\n",
    "            x = i(x)\n",
    "            x = F.relu(x)\n",
    "        # 출력층\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassModel(\n",
       "  (input_layer): Linear(in_features=5, out_features=50, bias=True)\n",
       "  (hidden_layer_list): ModuleList(\n",
       "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
       "    (1): Linear(in_features=30, out_features=20, bias=True)\n",
       "    (2): Linear(in_features=20, out_features=10, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=10, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = MulticlassModel(input_in=5, output_out=8, hidden_list=[50,30,20,10])\n",
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 기울기 소실 개념 및 해결 방법을 설명하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 손실값이 최소가 되는 지점을 찾는 과정에서 기울기를 계산하기 위해 미분을 하게 되는데 층의 개수가 많으면 가중치와 절편을 업데이트하는 역전파 과정에서 미분을 많이 하게 되어 기울기가 0이 되어버리는 상황이 발생하게 됩니다. 이러한 문제를 해결하기 위해 여러 활성화 함수가 고안되었으며 ReLU 함수가 그 대표적인 예입니다. 하지만 ReLU 함수로도 기울기 소실 문제가 완벽하게 해결되지 않아 Leaky ReLU와 같은 다양한 활성화 함수가 고안되고 있습니다. 기울기 소실 문제를 해결하는 완벽한 활성화 함수는 존재하지 않기 때문에 실제 딥러닝 시에는 다양한 활성화 함수를 사용해보고 결과를 비교해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 파이토치의 모델 동작 모드에 대해 관련 함수도 함께 설명하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치는 모델을 학습하거나 평가할때 동작하는 모드가 다릅니다. 모델을 훈련할때는 학습용 통계치를 사용하기 위해 명시적으로 model.train()을 선언한 뒤 zero_grad() (그레디언트 초기화), backward() (역전파 하면서 그레디언트 계산), step() (가중치, 절편 업데이트)을 통해 최적화를 진행해야 합니다. 또한, 검증이나 테스트 시에는 명시적으로 model.eval()을 선언하여 검증용 통계치를 사용하고 드롭아웃 비활성화 등을 진행한 뒤에 with torch.no_grad()를 통해 가중치 업데이트 없이 테스트를 진행해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
