{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> <hr> 모델 실습 <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SentenceClassifier 클래스는 임베딩 층을 구성할 떄 사용하는 단어 사전 크기(n_vocab)와 순환 신경망 클래스와 장단기 메모리 클래스에서 사용하는 매개변수를 입력으로 전달받는다.\n",
    "- 모델 종류(model_type) 매개변수로 순환 신경망을 사용할지, 장단기 메모리를 사용할지 설정한다.\n",
    "\n",
    "- 초기화 메서드에서는 SentenceClassifier 클래스에 입력된 함수의 매개변수에 따라 모델 구조를 미세 조정한다.\n",
    "- 분류기 계층은 모델을 양방향으로 구성한다면 전달되는 입력 채널 수가 달라지므로 분류기 계층을 현재 모델 구조에 맞게 변경한다.\n",
    "\n",
    "- 순방향 메서드에서는 입력받은 정수 인코딩을 임베딩 계층에 통과시켜 임베딩 값을 얻는다. 그리고 얻은 임베딩 값을 모델에 입ㅇ력하여 출력값을 얻는다.\n",
    "- 출력값(output)의 마지막 시점만 활용할 예정이므로 [:, -1, :]으로 마지막 시점의 결괏값만 분리해 분류기 계층에 전달한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분류 모델\n",
    "from torch import nn\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_vocab,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout=0.5,\n",
    "            bidirectional=True,\n",
    "            model_type=\"lstm\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding=nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type ==\"rnn\":\n",
    "            self.model=nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type ==\"lstm\":\n",
    "            self.model=nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        if bidirectional:\n",
    "            self.classifier=nn.Linear(hidden_dim * 2,1)\n",
    "        else:\n",
    "            self.classifier=nn.Linear(hidden_dim, 1)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeddings=self.embedding(inputs)\n",
    "        output, _=self.model(embeddings)\n",
    "        last_output=output[:, -1, :]\n",
    "        last_output=self.dropout(last_output)\n",
    "        logits=self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SentenceClassifier 클래스를 선언했다면 모델 학습에 사용할 데이터세트를 불러온다. 데이터세트는 코포라 라이브러리의 네이버 영화 리뷰 감정 분석 데이터세트를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\LG\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\LG\\Korpora\\nsmc\\ratings_test.txt\n",
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "# 데이터세트 불러오기\n",
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "#%pip install tabulate\n",
    "\n",
    "corpus=Korpora.load(\"nsmc\")\n",
    "corpus_df=pd.DataFrame(corpus.test)\n",
    "\n",
    "train_data=corpus_df.sample(frac=0.9, random_state=42)\n",
    "test_data=corpus_df.drop(train_data.index)\n",
    "\n",
    "print(train_data.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train_data))\n",
    "print(\"Testing Data Size :\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "# 데이터 토큰화 및 단어 사전 구축\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter=Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab=special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "tokenizer=Okt()\n",
    "train_tokens=[tokenizer.morphs(review) for review in train_data.text]\n",
    "test_tokens=[tokenizer.morphs(review) for review in test_data.text]\n",
    "\n",
    "vocab=build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "token_to_id={token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token={idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩 및 패딩\n",
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result=list()\n",
    "    for sequence in sequences:\n",
    "        sequence=sequence[:max_length]\n",
    "        pad_length=max_length-len(sequence)\n",
    "        padded_sequence=sequence+[pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "unk_id=token_to_id['<unk>']\n",
    "train_ids=[\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "]\n",
    "test_ids=[\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "]\n",
    "\n",
    "max_length=32\n",
    "pad_id=token_to_id[\"<pad>\"]\n",
    "train_ids=pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids=pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 적용\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids=torch.tensor(train_ids)\n",
    "test_ids=torch.tensor(test_ids)\n",
    "\n",
    "train_labels=torch.tensor(train_data.label.values, dtype=torch.float32)\n",
    "test_labels=torch.tensor(test_data.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset=TensorDataset(train_ids, train_labels)\n",
    "test_dataset=TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader=DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수와 최적화 함수 정의\n",
    "from torch import optim\n",
    "\n",
    "n_vocab=len(token_to_id)\n",
    "hidden_dim=64\n",
    "embedding_dim=128\n",
    "n_layers=2\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier=SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    ").to(device)\n",
    "criterion=nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer=optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6843744516372681\n",
      "Train Loss 500 : 0.6935779821135089\n",
      "Train Loss 1000 : 0.688331649853633\n",
      "Train Loss 1500 : 0.6736074270684269\n",
      "Train Loss 2000 : 0.6653966238384305\n",
      "Train Loss 2500 : 0.6562119460687404\n",
      "Val Loss: 0.6003798047384135, Val Accuracy : 0.6968\n",
      "Train Loss 0 : 0.417904794216156\n",
      "Train Loss 500 : 0.5810391278918869\n",
      "Train Loss 1000 : 0.5635296040838891\n",
      "Train Loss 1500 : 0.5472045288552132\n",
      "Train Loss 2000 : 0.536578912062683\n",
      "Train Loss 2500 : 0.521932823986399\n",
      "Val Loss: 0.46953022532379285, Val Accuracy : 0.7814\n",
      "Train Loss 0 : 0.35736462473869324\n",
      "Train Loss 500 : 0.4305190174225086\n",
      "Train Loss 1000 : 0.4259616179051218\n",
      "Train Loss 1500 : 0.4241200108267243\n",
      "Train Loss 2000 : 0.4193279863535792\n",
      "Train Loss 2500 : 0.4182979257636574\n",
      "Val Loss: 0.42167099158223065, Val Accuracy : 0.7986\n",
      "Train Loss 0 : 0.23584696650505066\n",
      "Train Loss 500 : 0.36406485047823417\n",
      "Train Loss 1000 : 0.3681068767334793\n",
      "Train Loss 1500 : 0.3697245377131417\n",
      "Train Loss 2000 : 0.3688083860217065\n",
      "Train Loss 2500 : 0.3684231408330642\n",
      "Val Loss: 0.4052914815922134, Val Accuracy : 0.8102\n",
      "Train Loss 0 : 0.3544546663761139\n",
      "Train Loss 500 : 0.325936604819136\n",
      "Train Loss 1000 : 0.3291078609908437\n",
      "Train Loss 1500 : 0.3274664942232233\n",
      "Train Loss 2000 : 0.32857568149474903\n",
      "Train Loss 2500 : 0.3313706119338878\n",
      "Val Loss: 0.39363918541528925, Val Accuracy : 0.8206\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 및 테스트\n",
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses=list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids=input_ids.to(device)\n",
    "        labels=labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits=model(input_ids)\n",
    "        loss=criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval==0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses=list()\n",
    "    corrects=list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids=input_ids.to(device)\n",
    "        labels=labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits=model(input_ids)\n",
    "        loss=criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat=torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f'Val Loss: {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}')\n",
    "\n",
    "epochs=5\n",
    "interval=500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [-0.40999418  1.6593337   1.1083715  -0.24211055  1.283378    1.310663\n",
      "  0.15313952 -0.42823     1.845338   -1.2713537  -2.4918156   0.5678896\n",
      "  2.0654538   0.350038    1.2830559   1.0796698  -1.4274625   0.3354824\n",
      " -0.231855   -0.9960354  -0.68714935  0.5737285   2.658381   -1.4902339\n",
      " -1.0307599  -0.7208531  -1.2912513  -0.7472857  -0.3688705   0.39077032\n",
      "  0.19352224 -1.2516541  -0.7396949   0.72476286 -1.312093    1.2689428\n",
      "  0.6009034  -1.0034074   0.11286522 -1.7167953  -0.81556237 -0.3037526\n",
      "  0.48595783  1.0280796   0.75985837 -0.27178425  0.65489763 -1.0963597\n",
      "  0.8265645  -0.23160197  3.617519    0.7064647   2.022217   -0.9964674\n",
      " -0.22540146  0.6039761   0.81769395  0.338092    0.6824848   0.91780096\n",
      " -0.01283852  0.8933017  -1.8751456  -0.9103439   0.51728314  1.4783889\n",
      " -3.0495174   1.7487664   0.38325652  0.7961067   1.1604681  -0.13437289\n",
      " -0.4388663  -0.562142   -0.5991527   0.43821457  0.2992261  -0.43645743\n",
      "  0.6861941   2.4266891   0.7141187  -0.24047372 -1.8588486   0.50020415\n",
      " -0.51719385  0.47074887  0.93485284 -1.2216717   0.15051268  0.14150113\n",
      " -1.60129     0.04638368 -0.77407485  1.3150344  -1.7332362   0.17952126\n",
      "  2.4512227   1.370807   -0.56649625  0.2938293   0.0976798   0.12198564\n",
      "  0.24164307  1.0459135  -1.2856133   0.28130007 -1.8367528   0.21937656\n",
      " -0.58533657 -1.5886467   1.6916337   1.5660446  -0.70712453 -0.66930944\n",
      "  0.9261482   1.9891688   0.69829005  0.01673742 -1.2929797  -0.28539822\n",
      " -1.9517428  -0.77664715  0.72475076 -0.8140232  -0.36614102 -1.7257344\n",
      "  0.62368685  0.347358  ]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델로부터 임베딩 추출\n",
    "token_to_embedding=dict()\n",
    "embedding_matrix=classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word]=emb\n",
    "\n",
    "token=vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp38-cp38-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\lg\\anaconda3\\envs\\text_018_230_38\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\lg\\anaconda3\\envs\\text_018_230_38\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lg\\anaconda3\\envs\\text_018_230_38\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lg\\anaconda3\\envs\\text_018_230_38\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.3.3-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 6.0/24.0 MB 33.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 13.9/24.0 MB 36.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.0/24.0 MB 30.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 21.8/24.0 MB 27.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 26.2 MB/s eta 0:00:00\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install gensim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018_230_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
