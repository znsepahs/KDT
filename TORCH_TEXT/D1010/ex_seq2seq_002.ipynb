{"cells":[{"cell_type":"markdown","metadata":{"id":"WLEtShDTBoOr"},"source":["### [ Seq2Seq 방식의 기계번역]\n","- 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델\n","- 기계 번역, 문장 생성, 질의응답, 메일 자동 응답 등에 활용되는 모델\n","- 번역: 입력 시퀀스와 출력 시퀀스를 각각 입력 문장과 번역 문장으로 생성\n","- 구성 : 인코더 + 디코더 + 생성기\n","    * 인코더 => 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤 마지막에 모든 단어 정보들 압축해서 하나의 벡터 즉, 컨텍스트 벡터(context vector) 생성\n","    * 컨텍스트 벡터(context vector) : 문장에 대한 정보가 응축\n","    * 인코더 => 디코로 전송\n","    * 디코더 => 컨텍스트 벡터를 활용하여 문장 생성\n","        * 훈련 단계 : 교사 강요(teacher forcing) 방식으로 디코더 모델 훈련\n","            - t-1의 실제 값을 입력값으로 사용\n","        * 테스트 단계 : 기존의 RNN 방식\n","- 병렬 코퍼스 데이터 : http://www.manythings.org/anki\n"]},{"cell_type":"markdown","metadata":{"id":"q9xUsp2yBoOv"},"source":["(1) 모듈 로딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTjzI3l7BoOv"},"outputs":[],"source":["import re\n","import os\n","import unicodedata\n","import numpy as np\n","import pandas as pd\n","import torch\n","from collections import Counter\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"markdown","metadata":{"id":"eyh62E7PBoOw"},"source":["### [1. 데이터 준비 ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZ6T_fe-BoOx"},"outputs":[],"source":["data_dir='../data/'\n","fra_file=data_dir+'fra.txt'\n","\n","num_samples = 33000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MskNEftXBoOx"},"outputs":[],"source":["if not os.path.exists(data_dir): os.makedirs(data_dir)"]},{"cell_type":"markdown","metadata":{"id":"tXEMkPR5BoOx"},"source":["### [ 2. 데이터 전처리 ]"]},{"cell_type":"markdown","metadata":{"id":"Q-gpKdZmBoOy"},"source":["===> 전처리 함수들"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPUoNQHPBoOy"},"outputs":[],"source":["# 코드 변환\n","def unicode_to_ascii(s):\n","  # 프랑스어 악센트(accent) 삭제\n","  # 예시 : 'déjà diné' -> deja dine\n","  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5neBTy2UBoOy"},"outputs":[],"source":["# 문장 전처리 함수\n","def preprocess_sentence(sent):\n","  # 악센트 삭제 함수 호출\n","  sent = unicode_to_ascii(sent.lower())\n","\n","  # 단어와 구두점 사이에 공백\n","  # Ex) \"he is a boy.\" => \"he is a boy .\"\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","  # 다수 개의 공백을 하나의 공백으로 치환\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53OlPr2UBoOz"},"outputs":[],"source":["# 파일 데이터를 영어 - 프랑스어로 분리 후 전처리 진행 함수\n","def load_preprocessed_data():\n","\n","  encoder_input, decoder_input, decoder_target = [], [], []\n","\n","  with open(fra_file, \"r\") as lines:\n","    for i, line in enumerate(lines):\n","      # source 데이터와 target 데이터 분리\n","      src_line, tar_line, _ = line.strip().split('\\t')\n","\n","      # source (영어) 데이터 전처리\n","      src_line = [w for w in preprocess_sentence(src_line).split()]\n","\n","      # target (프랑스어) 데이터 전처리\n","      tar_line = preprocess_sentence(tar_line)\n","      # 시작 & 끝 토큰 추가\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","\n","  return encoder_input, decoder_input, decoder_target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gyaCAX_DBoOz","outputId":"aafc540f-4145-4d5d-bb6a-eb05e347bcb7"},"outputs":[{"name":"stdout","output_type":"stream","text":["전처리 전 영어 문장 : Have you had dinner?\n","전처리 후 영어 문장 : have you had dinner ?\n","전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n","전처리 후 프랑스어 문장 : avez vous deja dine ?\n"]}],"source":["# 전처리 테스트\n","en_sent = u\"Have you had dinner?\"\n","fr_sent = u\"Avez-vous déjà diné?\"\n","\n","print('전처리 전 영어 문장 :', en_sent)\n","print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n","print('전처리 전 프랑스어 문장 :', fr_sent)\n","print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"]},{"cell_type":"markdown","metadata":{"id":"AR1c1XiYBoO0"},"source":["===> 입력 데이터 영어와 타겟 데이터 프랑스어 데이터 전처리 진행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHTS14ZmBoO0"},"outputs":[],"source":["sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPry23AwBoO0","outputId":"80504c0f-e30b-4039-aa06-ca8d94fdbc35"},"outputs":[{"name":"stdout","output_type":"stream","text":["인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n","디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n","디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"]}],"source":["print('인코더의 입력 :',sents_en_in[:5])\n","print('디코더의 입력 :',sents_fra_in[:5])\n","print('디코더의 레이블 :',sents_fra_out[:5])"]},{"cell_type":"markdown","metadata":{"id":"MJbdIj9EBoO0"},"source":["====> 단어 사전 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IIHQ92xGBoO1"},"outputs":[],"source":["# 토큰화 및 단어별 등장 빈도에 따른 정수 인코딩 진행 함수\n","def build_vocab(sents):\n","  word_list = []\n","\n","  for sent in sents:\n","      for word in sent:\n","        word_list.append(word)\n","\n","  # 각 단어별 등장 빈도를 계산하여 등장 빈도가 높은 순서로 정렬\n","  word_counts = Counter(word_list)\n","  vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","\n","  word_to_index = {}\n","  word_to_index['<PAD>'] = 0\n","  word_to_index['<UNK>'] = 1\n","\n","  # 등장 빈도가 높은 단어일수록 낮은 정수를 부여\n","  for index, word in enumerate(vocab) :\n","    word_to_index[word] = index + 2\n","\n","  return word_to_index"]},{"cell_type":"markdown","metadata":{"id":"6T8xK6R7BoO1"},"source":["====> 토큰화 및 단어사전 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQMpoZ6tBoO1"},"outputs":[],"source":["src_vocab = build_vocab(sents_en_in)\n","tar_vocab = build_vocab(sents_fra_in + sents_fra_out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_cjdO2PBoO1","outputId":"cd36075a-f8c3-4834-a9fc-d6b1f32f8120"},"outputs":[{"name":"stdout","output_type":"stream","text":["영어 단어 집합의 크기 : 4488, 프랑스어 단어 집합의 크기 : 7884\n"]}],"source":["src_vocab_size = len(src_vocab)\n","tar_vocab_size = len(tar_vocab)\n","print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"]},{"cell_type":"markdown","metadata":{"id":"V4xIThVxBoO1"},"source":["=====> 번역에 활용할 정수기반 단어 추출 데이터셋 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"euj0K-h9BoO1"},"outputs":[],"source":["index_to_src = {v: k for k, v in src_vocab.items()}\n","index_to_tar = {v: k for k, v in tar_vocab.items()}\n","\n","def texts_to_sequences(sents, word_to_index):\n","  encoded_X_data = []\n","  for sent in tqdm(sents):\n","    index_sequences = []\n","    for word in sent:\n","      try:\n","          index_sequences.append(word_to_index[word])\n","      except KeyError:\n","          index_sequences.append(word_to_index['<UNK>'])\n","    encoded_X_data.append(index_sequences)\n","  return encoded_X_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T10Tyu4MBoO1","outputId":"461a7092-ec1c-410c-ba79-955fec78b78c"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/33000 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 33000/33000 [00:00<00:00, 218546.12it/s]\n","100%|██████████| 33000/33000 [00:00<00:00, 124529.93it/s]\n","100%|██████████| 33000/33000 [00:00<00:00, 326736.30it/s]\n"]}],"source":["encoder_input = texts_to_sequences(sents_en_in, src_vocab)\n","decoder_input = texts_to_sequences(sents_fra_in, tar_vocab)\n","decoder_target = texts_to_sequences(sents_fra_out, tar_vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GnK8xkVqBoO2","outputId":"fda9f53b-899c-420f-f92b-9ba9cbd0764a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index: 0, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 1, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 2, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 3, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 4, 정수 인코딩 전: ['hi', '.'], 정수 인코딩 후: [742, 2]\n"]}],"source":["# 상위 5개의 샘플에 대해서 정수 인코딩 전, 후 문장 출력\n","# 인코더 입력이므로 <sos>나 <eos>가 없음\n","for i, (item1, item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n","    print(f\"Index: {i}, 정수 인코딩 전: {item1}, 정수 인코딩 후: {item2}\")"]},{"cell_type":"markdown","metadata":{"id":"mxfPOrV3BoO2"},"source":["====> 패딩\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_GeNlwPBoO2"},"outputs":[],"source":["# 번역이라 인코딩/디코딩 문장 길이 다름\n","def pad_sequences(sentences, max_len=None):\n","    # 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n","    if max_len is None:\n","        max_len = max([len(sentence) for sentence in sentences])\n","\n","    features = np.zeros((len(sentences), max_len), dtype=int)\n","    for index, sentence in enumerate(sentences):\n","        if len(sentence) != 0:\n","            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n","    return features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egzwrTXYBoO2"},"outputs":[],"source":["encoder_input = pad_sequences(encoder_input)\n","decoder_input = pad_sequences(decoder_input)\n","decoder_target = pad_sequences(decoder_target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MD1RXdi0BoO2","outputId":"5dab6cb6-a39a-4072-defe-f9d6edb5fb83"},"outputs":[{"name":"stdout","output_type":"stream","text":["인코더의 입력의 크기(shape) : (33000, 7)\n","디코더의 입력의 크기(shape) : (33000, 16)\n","디코더의 레이블의 크기(shape) : (33000, 16)\n"]}],"source":["print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"]},{"cell_type":"markdown","metadata":{"id":"xNfX13GOBoO2"},"source":["### [3. 학습/검증 데이터 ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6IU1tFrBoO3","outputId":"31632c64-b250-4589-cd6f-cd0510ff2765"},"outputs":[{"name":"stdout","output_type":"stream","text":["랜덤 시퀀스 : [20308  8155 17189 ... 29556 31343 29634]\n"]}],"source":["indices = np.arange(encoder_input.shape[0])\n","np.random.shuffle(indices)\n","print('랜덤 시퀀스 :',indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWZEN_vmBoO3"},"outputs":[],"source":["encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1SZ9DxVBoO3","outputId":"ab03955c-9744-43a7-bbab-3bdb92068794"},"outputs":[{"name":"stdout","output_type":"stream","text":["['is', 'this', 'ours', '?', '<PAD>', '<PAD>', '<PAD>']\n","['<sos>', 'est', 'ce', 'le', 'notre', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","['est', 'ce', 'le', 'notre', '?', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"]}],"source":["print([index_to_src[word] for word in encoder_input[30997]])\n","print([index_to_tar[word] for word in decoder_input[30997]])\n","print([index_to_tar[word] for word in decoder_target[30997]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlyMe1lNBoO3","outputId":"35c743f7-86fd-4e6c-f638-92a858a09d8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["검증 데이터의 개수 : 3300\n"]}],"source":["# 33,000개의 10%에 해당되는 3,300개의 데이터를 테스트 데이터로 사용\n","n_of_val = int(33000*0.1)\n","print('검증 데이터의 개수 :',n_of_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oGRCV73qBoO3"},"outputs":[],"source":["encoder_input_train = encoder_input[:-n_of_val]\n","decoder_input_train = decoder_input[:-n_of_val]\n","decoder_target_train = decoder_target[:-n_of_val]\n","\n","encoder_input_test = encoder_input[-n_of_val:]\n","decoder_input_test = decoder_input[-n_of_val:]\n","decoder_target_test = decoder_target[-n_of_val:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"emAZFBkJBoO4","outputId":"25548958-30ec-4baf-83d2-bd093c9c98bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 source 데이터의 크기 : (29700, 7)\n","훈련 target 데이터의 크기 : (29700, 16)\n","훈련 target 레이블의 크기 : (29700, 16)\n","테스트 source 데이터의 크기 : (3300, 7)\n","테스트 target 데이터의 크기 : (3300, 16)\n","테스트 target 레이블의 크기 : (3300, 16)\n"]}],"source":["print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"GXo6bJFrBoO4"},"source":["### [4. 기계번역기 모델 ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Y7zuIb0BoO4"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FsiJAODrBoO4"},"outputs":[],"source":["# 단어사전 ===> 임베딩 차원 수\n","embedding_dim = 256\n","\n","# RNN 층의 히든 유닛 수\n","hidden_units = 256"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMMw727RBoO4"},"outputs":[],"source":["# 인코더 클래스\n","#\n","class Encoder(nn.Module):\n","    def __init__(self, src_vocab_size, embedding_dim, hidden_units):\n","        super(Encoder, self).__init__()\n","        self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n","\n","    def forward(self, x):\n","        # x.shape == (batch_size, seq_len, embedding_dim)\n","        x = self.embedding(x)\n","        # hidden.shape == (1, batch_size, hidden_units), cell.shape == (1, batch_size, hidden_units)\n","        _, (hidden, cell) = self.lstm(x)\n","        # 인코더의 출력은 hidden state, cell state\n","        return hidden, cell"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JGkmkpFBoO7"},"outputs":[],"source":["# 디코더 클래스\n","\n","class Decoder(nn.Module):\n","    def __init__(self, tar_vocab_size, embedding_dim, hidden_units):\n","        super(Decoder, self).__init__()\n","        self.embedding = nn.Embedding(tar_vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n","        self.fc = nn.Linear(hidden_units, tar_vocab_size)\n","\n","    def forward(self, x, hidden, cell):\n","\n","        # x.shape == (batch_size, seq_len, embedding_dim)\n","        x = self.embedding(x)\n","\n","        # 디코더의 LSTM으로 인코더의 hidden state, cell state 전달.\n","        # output.shape == (batch_size, seq_len, hidden_units)\n","        # hidden.shape == (1, batch_size, hidden_units)\n","        # cell.shape == (1, batch_size, hidden_units)\n","        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n","\n","        # output.shape: (batch_size, seq_len, tar_vocab_size)\n","        output = self.fc(output)\n","\n","        # 디코더의 출력은 예측값, hidden state, cell state\n","        return output, hidden, cell"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jCYAP9VBoO7"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg):\n","        hidden, cell = self.encoder(src)\n","\n","        # 훈련 중에는 디코더의 출력 중 오직 output만 사용한다.\n","        output, _, _ = self.decoder(trg, hidden, cell)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRawNfTXBoO7"},"outputs":[],"source":["encoder = Encoder(src_vocab_size, embedding_dim, hidden_units)\n","decoder = Decoder(tar_vocab_size, embedding_dim, hidden_units)\n","model = Seq2Seq(encoder, decoder)\n","\n","loss_function = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = optim.Adam(model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3m-U9gZBoO8","outputId":"7e66d4fc-d68e-4886-f17d-d1fc8f25d25c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(4488, 256, padding_idx=0)\n","    (lstm): LSTM(256, 256, batch_first=True)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(7884, 256, padding_idx=0)\n","    (lstm): LSTM(256, 256, batch_first=True)\n","    (fc): Linear(in_features=256, out_features=7884, bias=True)\n","  )\n",")\n"]}],"source":["print(model)"]},{"cell_type":"markdown","metadata":{"id":"LcXKiMl1BoO8"},"source":["### [5. 학습 ]"]},{"cell_type":"markdown","metadata":{"id":"_2pY5pdXBoO8"},"source":["=====> 관련 함수들"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvG3hcdpBoO8"},"outputs":[],"source":["def evaluation(model, dataloader, loss_function, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_count = 0\n","\n","    with torch.no_grad():\n","        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n","            encoder_inputs = encoder_inputs.to(device)\n","            decoder_inputs = decoder_inputs.to(device)\n","            decoder_targets = decoder_targets.to(device)\n","\n","            # 순방향 전파\n","            # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n","            outputs = model(encoder_inputs, decoder_inputs)\n","\n","            # 손실 계산\n","            # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n","            # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n","            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n","            total_loss += loss.item()\n","\n","            # 정확도 계산 (패딩 토큰 제외)\n","            mask = decoder_targets != 0\n","            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n","            total_count += mask.sum().item()\n","\n","    return total_loss / len(dataloader), total_correct / total_count"]},{"cell_type":"markdown","metadata":{"id":"rvX1mthVBoO9"},"source":["====> 데이터 Tensor 화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLqfg7MxBoO9"},"outputs":[],"source":["# encoder_input_train => (29700,7)\n","# decoder_input_train => (29700,16)\n","# decoder_target_train => (29700,16)\n","encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n","decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n","decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n","\n","# encoder_input_test => (3300,7)\n","# decoder_input_test => (3300,16)\n","# decoder_target_test => (3300,16)\n","encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n","decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n","decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)"]},{"cell_type":"markdown","metadata":{"id":"t-pk7qAdBoO9"},"source":["====> 데이터셋과 로더 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLuasqzGBoO-"},"outputs":[],"source":["# 데이터셋 및 데이터로더 생성\n","batch_size = 128\n","\n","train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"pkToylQWBoO-"},"source":["====> 학습 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSxiRuVFBoO-","outputId":"2f4526ba-5fa2-4189-dfe9-01b8d6f9d7b7"},"outputs":[{"data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(4488, 256, padding_idx=0)\n","    (lstm): LSTM(256, 256, batch_first=True)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(7884, 256, padding_idx=0)\n","    (lstm): LSTM(256, 256, batch_first=True)\n","    (fc): Linear(in_features=256, out_features=7884, bias=True)\n","  )\n",")"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["num_epochs = 30\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLOAI00uBoO-"},"outputs":[],"source":["def training():\n","    # Training loop\n","    best_val_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","        # 훈련 모드\n","        model.train()\n","\n","        for encoder_inputs, decoder_inputs, decoder_targets in train_dataloader:\n","            encoder_inputs = encoder_inputs.to(device)\n","            decoder_inputs = decoder_inputs.to(device)\n","            decoder_targets = decoder_targets.to(device)\n","\n","            # 기울기 초기화\n","            optimizer.zero_grad()\n","\n","            # 순방향 전파\n","            # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n","            outputs = model(encoder_inputs, decoder_inputs)\n","\n","            # 손실 계산 및 역방향 전파\n","            # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n","            # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n","            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n","            loss.backward()\n","\n","            # 가중치 업데이트\n","            optimizer.step()\n","\n","        train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n","        valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n","\n","        print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n","\n","        # 검증 손실이 최소일 때 체크포인트 저장\n","        if valid_loss < best_val_loss:\n","            print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n","            best_val_loss = valid_loss\n","            torch.save(model.state_dict(), 'best_model_checkpoint.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-4G825oBoO-","outputId":"cf1958e7-5e86-4ce3-8988-c0729620000a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1/30 | Train Loss: 2.9146 | Train Acc: 0.5345 | Valid Loss: 3.0165 | Valid Acc: 0.5375\n","Validation loss improved from inf to 3.0165. 체크포인트를 저장합니다.\n","Epoch: 2/30 | Train Loss: 2.2601 | Train Acc: 0.6046 | Valid Loss: 2.4858 | Valid Acc: 0.5961\n","Validation loss improved from 3.0165 to 2.4858. 체크포인트를 저장합니다.\n","Epoch: 3/30 | Train Loss: 1.8551 | Train Acc: 0.6478 | Valid Loss: 2.2106 | Valid Acc: 0.6256\n","Validation loss improved from 2.4858 to 2.2106. 체크포인트를 저장합니다.\n","Epoch: 4/30 | Train Loss: 1.5415 | Train Acc: 0.6854 | Valid Loss: 2.0139 | Valid Acc: 0.6481\n","Validation loss improved from 2.2106 to 2.0139. 체크포인트를 저장합니다.\n","Epoch: 5/30 | Train Loss: 1.2991 | Train Acc: 0.7196 | Valid Loss: 1.8790 | Valid Acc: 0.6653\n","Validation loss improved from 2.0139 to 1.8790. 체크포인트를 저장합니다.\n","Epoch: 6/30 | Train Loss: 1.0877 | Train Acc: 0.7577 | Valid Loss: 1.7716 | Valid Acc: 0.6804\n","Validation loss improved from 1.8790 to 1.7716. 체크포인트를 저장합니다.\n","Epoch: 7/30 | Train Loss: 0.9040 | Train Acc: 0.7889 | Valid Loss: 1.6921 | Valid Acc: 0.6884\n","Validation loss improved from 1.7716 to 1.6921. 체크포인트를 저장합니다.\n","Epoch: 8/30 | Train Loss: 0.7470 | Train Acc: 0.8239 | Valid Loss: 1.6231 | Valid Acc: 0.6998\n","Validation loss improved from 1.6921 to 1.6231. 체크포인트를 저장합니다.\n","Epoch: 9/30 | Train Loss: 0.6232 | Train Acc: 0.8550 | Valid Loss: 1.5742 | Valid Acc: 0.7085\n","Validation loss improved from 1.6231 to 1.5742. 체크포인트를 저장합니다.\n","Epoch: 10/30 | Train Loss: 0.5301 | Train Acc: 0.8734 | Valid Loss: 1.5421 | Valid Acc: 0.7147\n","Validation loss improved from 1.5742 to 1.5421. 체크포인트를 저장합니다.\n","Epoch: 11/30 | Train Loss: 0.4483 | Train Acc: 0.8882 | Valid Loss: 1.5268 | Valid Acc: 0.7179\n","Validation loss improved from 1.5421 to 1.5268. 체크포인트를 저장합니다.\n","Epoch: 12/30 | Train Loss: 0.3909 | Train Acc: 0.8999 | Valid Loss: 1.5143 | Valid Acc: 0.7175\n","Validation loss improved from 1.5268 to 1.5143. 체크포인트를 저장합니다.\n","Epoch: 13/30 | Train Loss: 0.3409 | Train Acc: 0.9067 | Valid Loss: 1.5182 | Valid Acc: 0.7203\n","Epoch: 14/30 | Train Loss: 0.3036 | Train Acc: 0.9134 | Valid Loss: 1.5234 | Valid Acc: 0.7229\n","Epoch: 15/30 | Train Loss: 0.2770 | Train Acc: 0.9184 | Valid Loss: 1.5266 | Valid Acc: 0.7221\n","Epoch: 16/30 | Train Loss: 0.2526 | Train Acc: 0.9221 | Valid Loss: 1.5289 | Valid Acc: 0.7254\n","Epoch: 17/30 | Train Loss: 0.2321 | Train Acc: 0.9246 | Valid Loss: 1.5518 | Valid Acc: 0.7238\n","Epoch: 18/30 | Train Loss: 0.2190 | Train Acc: 0.9262 | Valid Loss: 1.5534 | Valid Acc: 0.7249\n","Epoch: 19/30 | Train Loss: 0.2073 | Train Acc: 0.9279 | Valid Loss: 1.5688 | Valid Acc: 0.7239\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[43], line 25\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 손실 계산 및 역방향 전파\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), decoder_targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 가중치 업데이트\u001b[39;00m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[1;32mc:\\Users\\anece\\anaconda3\\envs\\TORCH_NLP\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anece\\anaconda3\\envs\\TORCH_NLP\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anece\\anaconda3\\envs\\TORCH_NLP\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["training()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neANuZWrBoO-"},"outputs":[],"source":["# 모델 로드\n","model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n","\n","# 모델을 device에 올립니다.\n","model.to(device)\n","\n","# 검증 데이터에 대한 정확도와 손실 계산\n","val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ct62LtYHBoO_","outputId":"5738840f-a201-4d2c-9ba5-9701ff47822c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best model validation loss: 1.5409\n","Best model validation accuracy: 0.7179\n"]}],"source":["\n","print(f'Best model validation loss: {val_loss:.4f}')\n","print(f'Best model validation accuracy: {val_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pf20MwGKBoO_","outputId":"dab3a9ce-6387-4610-f3bd-2cb670c8d6e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["3\n","4\n"]}],"source":["print(tar_vocab['<sos>'])\n","print(tar_vocab['<eos>'])"]},{"cell_type":"markdown","metadata":{"id":"He40HcxFBoO_"},"source":["#### 기계   번역 테스트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_YA-nyfBoO_"},"outputs":[],"source":["index_to_src = {v: k for k, v in src_vocab.items()}\n","index_to_tar = {v: k for k, v in tar_vocab.items()}\n","\n","# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0 and encoded_word != tar_vocab['<sos>'] and encoded_word != tar_vocab['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"du2XuAuoBoO_","outputId":"f06a617d-98d2-46d9-d069-8a15ecfffe0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[  3 211  10 126   2   0   0]\n","[  3  13  17 155  25 245   2   0   0   0   0   0   0   0   0   0]\n","[ 13  17 155  25 245   2   4   0   0   0   0   0   0   0   0   0]\n"]}],"source":["print(encoder_input_test[25])\n","print(decoder_input_test[25])\n","print(decoder_target_test[25])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9uLs5qMBoPA"},"outputs":[],"source":["def decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, max_output_len, int_to_src_token, int_to_tar_token):\n","    encoder_inputs = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n","\n","    # 인코더의 초기 상태 설정\n","    hidden, cell = model.encoder(encoder_inputs)\n","\n","    # 시작 토큰 <sos>을 디코더의 첫 입력으로 설정\n","    # unsqueeze(0)는 배치 차원을 추가하기 위함.\n","    decoder_input = torch.tensor([3], dtype=torch.long).unsqueeze(0).to(device)\n","\n","    decoded_tokens = []\n","\n","    # for문을 도는 것 == 디코더의 각 시점\n","    for _ in range(max_output_len):\n","        output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n","\n","        # 소프트맥스 회귀를 수행. 예측 단어의 인덱스\n","        output_token = output.argmax(dim=-1).item()\n","\n","        # 종료 토큰 <eos>\n","        if output_token == 4:\n","            break\n","\n","        # 각 시점의 단어(정수)는 decoded_tokens에 누적하였다가 최종 번역 시퀀스로 리턴\n","        decoded_tokens.append(output_token)\n","\n","        # 현재 시점의 예측. 다음 시점의 입력으로 사용된다.\n","        decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n","\n","    return ' '.join(int_to_tar_token[token] for token in decoded_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iEKAietjBoPA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8YtqXPSBoPA","outputId":"9a154870-6065-48a0-aef5-80a241aace05"},"outputs":[{"name":"stdout","output_type":"stream","text":["입력문장 : solve the problem . \n","정답문장 : resous le probleme . \n","번역문장 : resous le probleme .\n","--------------------------------------------------\n","입력문장 : answer tom . \n","정답문장 : repondez a tom . \n","번역문장 : repondez a tom .\n","--------------------------------------------------\n","입력문장 : this is my fault . \n","정답문장 : c est de ma faute . \n","번역문장 : c est de ma faute .\n","--------------------------------------------------\n","입력문장 : it s a conspiracy . \n","정답문장 : c est une conspiration . \n","번역문장 : c est une conspiration .\n","--------------------------------------------------\n","입력문장 : i like that . \n","정답문장 : j aime cela . \n","번역문장 : ca me plait .\n","--------------------------------------------------\n"]}],"source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_train[seq_index]\n","  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"번역문장 :\",translated_text)\n","  print(\"-\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QudGd1N1BoPA","outputId":"9202a82a-021a-41cc-b6e8-bd0e6d2ecc25"},"outputs":[{"name":"stdout","output_type":"stream","text":["입력문장 : tom looks curious . \n","정답문장 : tom a l air curieux . \n","번역문장 : tom semble curieux .\n","--------------------------------------------------\n","입력문장 : tom looks awkward . \n","정답문장 : tom a l air maladroit . \n","번역문장 : tom a l air un mal de tete .\n","--------------------------------------------------\n","입력문장 : that s so sad . \n","정답문장 : c est si triste . \n","번역문장 : c est si triste .\n","--------------------------------------------------\n","입력문장 : tom is cynical . \n","정답문장 : tom est cynique . \n","번역문장 : tom est reste debout .\n","--------------------------------------------------\n","입력문장 : where s my driver ? \n","정답문장 : ou est mon chauffeur ? \n","번역문장 : ou est mon peigne ?\n","--------------------------------------------------\n"]}],"source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_test[seq_index]\n","  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n","  print(\"번역문장 :\",translated_text)\n","  print(\"-\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cx68TivpBoPA"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"TEXT_11_110_38","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}