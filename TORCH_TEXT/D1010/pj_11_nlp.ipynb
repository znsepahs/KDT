{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_12248\\1179179661.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if not hasattr(collections, 'Callable'):\n"
     ]
    }
   ],
   "source": [
    "import collections.abc\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# soup error 방지\n",
    "import collections\n",
    "if not hasattr(collections, 'Callable'):\n",
    "    collections.Callable = collections.abc.Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 1만개 크롤링\n",
    "for i in range(1,1000):  \n",
    "    url =   f'https://kin.naver.com/search/list.naver?query=%EC%9D%B4%EB%B9%84%EC%9D%B8%ED%9B%84%EA%B3%BC&page={i}'\n",
    "    html   =   urlopen(url)\n",
    "    soup   =   BeautifulSoup(html.read(),   'html.parser')\n",
    "    links = soup.find('ul', class_='basic1').find_all('a')\n",
    "\n",
    "\n",
    "    links_list = []\n",
    "    for link in links:\n",
    "        if ('https://kin.naver.com/qna/detail.naver?d1id' in link['href']) and (link['href'] not in links_list):\n",
    "            links_list.append(link['href'])\n",
    "\n",
    "    # links_list = [link['href'] for link in links if 'https://kin.naver.com/qna/detail.naver?d1id' in link['href']]\n",
    "\n",
    "    # 링크 출력 (최대 10개)\n",
    "    # for href in links_list:\n",
    "    #     print(f\"{href}\")\n",
    "\n",
    "    for idx, url in enumerate(links_list):\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # HTML을 BeautifulSoup으로 파싱\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        soup = soup.find('div', {'class':'questionDetail'})\n",
    "        soup = soup.get_text(separator=' ').strip()\n",
    "        FILE_PATH = '../../../LocalData/nlp/pj_data/'\n",
    "        f = open(FILE_PATH+f\"{i}, {idx}.txt\", 'w')\n",
    "        # print(i, idx, soup)\n",
    "        f.write(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textDF = pd.DataFrame(columns=['txt','label'])\n",
    "\n",
    "# 본인 데이터 파일 지정\n",
    "TRAIN_PATH = '../../../LocalData/nlp/otolaryngology_data/'\n",
    "files = os.listdir(TRAIN_PATH)\n",
    "idx = 0\n",
    "for file in files:\n",
    "    with open(TRAIN_PATH+file, mode='r', encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "        text = re.sub('[^가-힣]+',' ',text)\n",
    "        textDF.loc[idx] = [text,1]\n",
    "        idx+=1\n",
    "        print(idx)\n",
    "        # 전체 인덱스 지정용 idx\n",
    "\n",
    "# 내과\n",
    "i_idx=1\n",
    "TRAIN_PATH = '../../../LocalData/nlp/medicine_data/'\n",
    "files = os.listdir(TRAIN_PATH)\n",
    "for file in files:\n",
    "    with open(TRAIN_PATH+file, mode='r', encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "        text = re.sub('[^가-힣]+',' ',text)\n",
    "        textDF.loc[idx] = [text,0]\n",
    "        i_idx+=1\n",
    "        idx+=1\n",
    "        print(f\"idx : {idx}, n_idx : {i_idx}\")\n",
    "        if i_idx == 3300: break\n",
    "        # i_idx 는 inner_data용 idx 3300개만 뽑기 위한 용도, 아래 n_idx, m_idx 또한 같은 용도. 참고\n",
    "\n",
    "# 정신과용\n",
    "m_idx=1\n",
    "TRAIN_PATH = '../../../LocalData/nlp/psychiatry_data/'\n",
    "files = os.listdir(TRAIN_PATH)\n",
    "for file in files:\n",
    "    with open(TRAIN_PATH+file, mode='r', encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "        text = re.sub('[^가-힣]+',' ',text)\n",
    "        textDF.loc[idx] = [text,0]\n",
    "        m_idx+=1\n",
    "        idx+=1\n",
    "        print(f\"idx : {idx}, n_idx : {m_idx}\")\n",
    "        if m_idx == 3300: break\n",
    "\n",
    "\n",
    "# 안과용\n",
    "n_idx=1\n",
    "TRAIN_PATH = '../../../LocalData/nlp/eyes_data/'\n",
    "files = os.listdir(TRAIN_PATH)\n",
    "for file in files:\n",
    "    with open(TRAIN_PATH+file, mode='r', encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "        text = re.sub('[^가-힣]+',' ',text)\n",
    "        textDF.loc[idx] = [text,0]\n",
    "        n_idx+=1\n",
    "        idx+=1\n",
    "        print(f\"idx : {idx}, n_idx : {n_idx}\")\n",
    "        if n_idx == 3300: break\n",
    "\n",
    "\n",
    "# 종료시 알림\n",
    "import winsound as sd\n",
    "def beepsound():\n",
    "    fr = 2000    # range : 37 ~ 32767\n",
    "    du = 300     # 1000 ms ==1second\n",
    "    sd.Beep(fr, du)\n",
    "beepsound()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분류 모델\n",
    "from torch import nn\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_vocab,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout=0.5,\n",
    "            bidirectional=True,\n",
    "            model_type=\"lstm\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding=nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type ==\"rnn\":\n",
    "            self.model=nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type ==\"lstm\":\n",
    "            self.model=nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        if bidirectional:\n",
    "            self.classifier=nn.Linear(hidden_dim * 2,1)\n",
    "        else:\n",
    "            self.classifier=nn.Linear(hidden_dim, 1)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeddings=self.embedding(inputs)\n",
    "        output, _=self.model(embeddings)\n",
    "        last_output=output[:, -1, :]\n",
    "        last_output=self.dropout(last_output)\n",
    "        logits=self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | txt                                                                                                                                                                                                  |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------:|\n",
      "|  1430 | 이비인후과 집 가까이 있는곳 가려는데 팔달시장에 수이비인후과 소아청소년만 보나요                                                                                                                     |       1 |\n",
      "| 16400 | 고 학생인데 중학생때부터 집중도 통 안되고 망상에 빠지는 경우가 너무 많아서 검사를 한 번 해보려고 하는데요 검색해보니 판정받으면 정신과 기록이 남는다 하던데 혹시 이게 나중에 악영향을 끼칠 수 있나요 |       0 |\n",
      "|  8144 | 후비루 치료 잘하는 이비인후과                                                                                                                                                                        |       1 |\n",
      "|  4610 | 이비인후과 집 가까이 있는곳 가려는데 팔달시장에 수이비인후과 소아청소년만 보나요                                                                                                                     |       1 |\n",
      "| 16292 | 맨 위에 있는건 뒤에 반으로 선이 있고 나머지는 맨들맨등해요 의사선생님이 잠 잘오게 한다는 약이라고했느제 뭔약인지 이름 알려주세요                                                                     |       0 |\n",
      "Training Data Size : 17898\n",
      "Testing Data Size : 1989\n"
     ]
    }
   ],
   "source": [
    "# 데이터세트 불러오기\n",
    "train_data=textDF.sample(frac=0.9, random_state=42)\n",
    "test_data=textDF.drop(train_data.index)\n",
    "\n",
    "print(train_data.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train_data))\n",
    "print(\"Testing Data Size :\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '이', '에', '가', '이비인후과', '를', '을', '는', '도']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "# 데이터 토큰화 및 단어 사전 구축\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter=Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab=special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "tokenizer=Okt()\n",
    "train_tokens=[tokenizer.morphs(review) for review in train_data.txt]\n",
    "test_tokens=[tokenizer.morphs(review) for review in test_data.txt]\n",
    "\n",
    "vocab=build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "token_to_id={token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token={idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 사전 csv 저장\n",
    "vocab_df = pd.DataFrame(columns=['key', 'value'])\n",
    "vocab_df['key'] = token_to_id.keys()\n",
    "vocab_df['value'] = token_to_id.values()\n",
    "vocab_df.to_csv('../../../LocalData/nlp/my_vocab/nose_vocab.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5 209 391 146 143 101 344 137 407   3  30   5 180 264  51 392  13   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[ 850 2695    3 1097    2    9  391    1  195 1021 1791    1  793    5\n",
      "   44 1053    9 1280 2745  147    1    1   68   32  508    1 2116 2052\n",
      " 2869    1  196 1403]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩 및 패딩\n",
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result=list()\n",
    "    for sequence in sequences:\n",
    "        sequence=sequence[:max_length]\n",
    "        pad_length=max_length-len(sequence)\n",
    "        padded_sequence=sequence+[pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "unk_id=token_to_id['<unk>']\n",
    "train_ids=[\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "]\n",
    "test_ids=[\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "]\n",
    "\n",
    "max_length=32\n",
    "pad_id=token_to_id[\"<pad>\"]\n",
    "train_ids=pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids=pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 적용\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids=torch.tensor(train_ids)\n",
    "test_ids=torch.tensor(test_ids)\n",
    "\n",
    "train_labels=torch.tensor(train_data.label.values, dtype=torch.float32)\n",
    "test_labels=torch.tensor(test_data.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset=TensorDataset(train_ids, train_labels)\n",
    "test_dataset=TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader=DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수와 최적화 함수 정의\n",
    "from torch import optim\n",
    "\n",
    "n_vocab=len(token_to_id)\n",
    "hidden_dim=64\n",
    "embedding_dim=128\n",
    "n_layers=2\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier=SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    ").to(device)\n",
    "criterion=nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer=optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수\n",
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses=list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids=input_ids.to(device)\n",
    "        labels=labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits=model(input_ids)\n",
    "        loss=criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval==0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 테스트 함수\n",
    "def test(model, datasets, criterion, device, min_loss, max_score, trigger, epoch):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "    t_loss = np.mean(losses)\n",
    "    t_score = np.mean(corrects)\n",
    "\n",
    "    SAVE_FILE = f'../../../LocalData/nlp/models/model_train_wbs_{epoch}_{t_score}.pth' \n",
    "    SAVE_MODEL = f'../../../LocalData/nlp/models/model_train_all_{epoch}_{t_score}.pth' \n",
    "\n",
    "    if min_loss < t_loss:\n",
    "        trigger+=1\n",
    "        print('loss 상승')\n",
    "    else : \n",
    "        trigger=0\n",
    "        min_loss = t_loss\n",
    "    \n",
    "    if max_score < t_score:\n",
    "        torch.save(model.state_dict() ,SAVE_FILE)\n",
    "        torch.save(model, SAVE_MODEL)\n",
    "    print(f\"Val Loss : {t_loss}, Val Accuracy : {t_score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 1.5970624822614354e-10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m min_loss, max_score, trigger\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     test(classifier, test_loader, criterion, device, min_loss, max_score, trigger, epoch)\n",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, datasets, criterion, optimizer, device, interval)\u001b[0m\n\u001b[0;32m     12\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m interval\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\LG\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LG\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LG\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "interval=500\n",
    "\n",
    "min_loss, max_score, trigger=100, -1, 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device, min_loss, max_score, trigger, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스펜서 [-1.17756     1.3826123  -0.9393518  -0.5108446   0.20590624  0.4730828\n",
      " -0.15738548  0.91913676 -1.5182947   1.3937839   1.1660508  -1.7804289\n",
      "  1.0866077   0.35119864 -0.62764955  0.8202577  -0.8918364   0.2951318\n",
      "  0.7314091  -0.09585405 -0.5506177  -0.02704389 -0.89259285  0.42719123\n",
      "  0.21297316  1.4330459   0.35770783  0.56020284  1.757232    1.271189\n",
      " -0.40390894 -0.58911556  0.97709715 -1.5738097  -0.14241177  0.19763255\n",
      " -2.2133343   0.5757457   1.2171783   0.35146493 -0.36045954  0.710397\n",
      "  0.3819382  -0.31749526 -1.0158403  -0.8202974  -0.2042847  -0.5847889\n",
      " -0.3832918   0.17157674  0.38590717 -1.8761234  -0.07825271  1.8193064\n",
      " -0.5564737  -0.6393762  -0.23808424 -0.8561183   0.25855374 -1.1698513\n",
      "  0.3700656   0.21001329  0.6314755  -1.3975614   1.2847075   0.96625614\n",
      " -0.59797823  1.1720486   0.1980955  -1.2359601  -1.2676605   0.7555174\n",
      " -0.5192259   0.6573489   0.88628775  0.9962074  -0.29819015 -0.19083454\n",
      " -0.34732282 -0.6416058  -0.02120762  1.218884   -2.2064035  -0.25146407\n",
      " -0.11565769  0.06404614 -1.8410519  -0.6476922  -0.94396144 -0.29094672\n",
      "  0.4633285  -0.3856965   1.4387735   0.34099764  0.2560061   0.13965358\n",
      " -0.33674315 -0.5028556  -0.70551413  0.12880649  0.9991533  -0.49327898\n",
      " -1.6255083   0.90599966 -0.2849199   1.6089541   0.5153151  -0.8891785\n",
      " -1.0666263   0.44096214 -1.0980011   0.4911688   0.6312306   1.4042227\n",
      "  0.29095533  2.042035   -0.978049   -0.44280082 -0.36339715 -0.13314293\n",
      " -1.0805964   1.1358824  -0.47975594  1.1437023   0.28955352 -0.2893687\n",
      "  0.5095568  -0.6408818 ]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델로부터 임베딩 추출\n",
    "token_to_embedding=dict()\n",
    "embedding_matrix=classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word]=emb\n",
    "\n",
    "token=vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_TRANSLATE = {0:'이비인후과는 아닌듯...', 1:'이비인후과'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x_data, y_data):\n",
    "    # 텐서 하나\n",
    "    # y_data의 차원을 확인하고 unsqueeze가 필요한지 확인\n",
    "    if len(y_data.shape) == 1:  # 1D 텐서인 경우\n",
    "        y_data = y_data.unsqueeze(1)  # (N,) -> (N, 1)\n",
    "    \n",
    "    y_data = y_data.float()  # float형으로 변환\n",
    "    x_data = x_data.unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        # 예측 수행\n",
    "        pred = model(x_data)\n",
    "        pred=torch.sigmoid(pred)\n",
    "        # 0.5 이상이면 1, 미만이면 0으로 변환\n",
    "        pred_labels = torch.argmax((pred >= 0.5).int())\n",
    "        pred_labels = [LABEL_TRANSLATE[int(label)] for label in pred_labels.flatten()]\n",
    "        real_labels = [LABEL_TRANSLATE[int(label)] for label in y_data.flatten()]\n",
    "\n",
    "    return pred_labels, real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model=torch.load('../../../LocalData/nlp/models/model_train_all_9_0.9904474610356964.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 65,   8, 111,   8,  23,  94, 123,   7,  97,  25, 325,  99,  50,   5,\n",
      "         11,  94, 123,   7,  97, 110, 182,  11,   4, 124, 303,  57,  16,  36,\n",
      "        329,   7, 355, 334], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.tensors[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측: 이비인후과\n",
      "결과: 이비인후과\n",
      "\n",
      "예측: 이비인후과\n",
      "결과: 이비인후과\n",
      "\n",
      "예측: 이비인후과\n",
      "결과: 이비인후과\n",
      "\n",
      "예측: 이비인후과\n",
      "결과: 이비인후과\n",
      "\n",
      "예측: 이비인후과\n",
      "결과: 이비인후과\n",
      "\n",
      "예측: 이비인후과\n",
      "결과: 이비인후과\n",
      "\n",
      "예측: 이비인후과\n",
      "결과: 이비인후과\n",
      "\n",
      "예측: 이비인후과\n",
      "결과: 이비인후과\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_list=[5,8,13,17,60,77,404,800]\n",
    "\n",
    "for num in num_list:\n",
    "    pred_label, real_label = predict(lstm_model, test_dataset.tensors[0][num], test_dataset.tensors[1][num])\n",
    "\n",
    "    print(f\"예측: {pred_label[0]}\")\n",
    "    print(f\"결과: {real_label[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018_230_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
